{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd40f4f4-5a80-40b7-94b3-aed136daa9d8",
   "metadata": {},
   "source": [
    "# Process the Load Data for the NAERM Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e597e9-538f-41d4-afa9-a268b10ab1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing the packages we need:\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f6fe5-0769-4298-887e-7c4cffc64a0d",
   "metadata": {},
   "source": [
    "## Set the Directory Structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd887600-32da-46dc-b411-fa63c75f564f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Identify the data input and image output directories:\n",
    "tell_data_input_dir =  '/Users/burl878/Documents/Code/code_repos/naerm_heat_wave_loads/data/TELL_Data/'\n",
    "data_output_dir =  '/Users/burl878/Documents/Code/code_repos/naerm_heat_wave_loads/data/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed79da3-d4b8-4ab7-88f7-0d420eb490ef",
   "metadata": {},
   "source": [
    "## Suppress Future Warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea3ce0d-f70d-4541-b17c-04ab2e00ffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78893b17-7c20-4ea5-8292-90dea6799307",
   "metadata": {},
   "source": [
    "## Create a Function to Process the 2035 GridView Data Used in Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f99e5c4-6575-4d2e-843b-21064c017f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gridview_data(data_input_dir: str):\n",
    "    # Read in the raw data .csv file:\n",
    "    gv_df = pd.read_csv((data_input_dir + '2021_1_Heatwave_Load_stress.csv'))\n",
    "\n",
    "    # Subset to just the annual total demand by BA:\n",
    "    gv_df = gv_df[-3:-2]\n",
    "       \n",
    "    # Strip the unecessary bits from the column names:\n",
    "    gv_df.columns = gv_df.columns.str.replace(\"_2030.dat\", \"\")\n",
    "    gv_df.columns = gv_df.columns.str.replace(\"Load_\", \"\")\n",
    "       \n",
    "    # Delete the index column:\n",
    "    del gv_df[\"Index\"] \n",
    "    \n",
    "    # Convert the values to floats:\n",
    "    gv_df = gv_df.astype('float64')\n",
    "    \n",
    "    # Compute the total loads for CISO, IPCO, NEVP, and PACE:\n",
    "    gv_df['CISO'] = (gv_df['CIPB'] + gv_df['CIPV'] + gv_df['CISC'] + gv_df['CISD'] + gv_df['VEA']).round(2)\n",
    "    gv_df['IPCO'] = (gv_df['IPFE'] + gv_df['IPMV'] + gv_df['IPTV']).round(2)\n",
    "    gv_df['PACE'] = (gv_df['PAID'] + gv_df['PAUT'] + gv_df['PAWY']).round(2)\n",
    "    gv_df['NEVP_Sum'] = (gv_df['NEVP'] + gv_df['SPPC']).round(2)\n",
    "           \n",
    "    # Rename a few columns for consistency:\n",
    "    gv_df.rename(columns={'CIPB': 'CISO_CIPB', 'CIPV': 'CISO_CIPV', 'CISC': 'CISO_CISC', 'CISD': 'CISO_CISD', 'VEA': 'CISO_VEA',\n",
    "                          'IPFE': 'IPCO_IPFE', 'IPMV': 'IPCO_IPMV', 'IPTV': 'IPCO_IPTV',\n",
    "                          'NEVP': 'NEVP_NEVP', 'SPPC': 'NEVP_SPPC',\n",
    "                          'PAID': 'PACE_PAID', 'PAUT': 'PACE_PAUT', 'PAWY': 'PACE_PAWY'}, inplace=True) \n",
    "    gv_df.rename(columns={'NEVP_Sum': 'NEVP'}, inplace=True) \n",
    "    \n",
    "    # Squeeze the dataframe:\n",
    "    gv_df = gv_df.squeeze().to_frame()\n",
    "        \n",
    "    # Rename the columns:\n",
    "    gv_df.reset_index(inplace=True)\n",
    "    gv_df = gv_df.rename(columns = {'index':'BA'})\n",
    "    gv_df.rename(columns={gv_df.columns[1]: \"Total_Load_MWh\" }, inplace = True)\n",
    "       \n",
    "    # Sort the dataframe alphabetically by BA name:\n",
    "    gv_df = gv_df.sort_values('BA')\n",
    "    \n",
    "    # Return the output dataframe:\n",
    "    return gv_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9778e2e-41a0-4a77-b985-40a3cfaba372",
   "metadata": {},
   "source": [
    "## Create a Function to Aggregate the Raw TELL MLP Output into a Single Dataframe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4dd356-95b4-43e8-987a-1012754a25b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_mlp_output_files(tell_data_input_dir: str, year_to_process: str):\n",
    "    \n",
    "    # Create a list of all of the MLP output files in the \"mlp_input_dir\" and aggregate the files in that list:\n",
    "    list_of_files = sorted(glob.glob(os.path.join(tell_data_input_dir, year_to_process, '*_mlp_output.csv')))\n",
    "\n",
    "    # Loop over the list of MLP output files:\n",
    "    for file in range(len(list_of_files)):\n",
    "\n",
    "        # Read in the .csv file and replace missing values with nan:\n",
    "        mlp_data = pd.read_csv(list_of_files[file]).replace(-9999, np.nan)\n",
    "\n",
    "        # Rename the \"Load\" variable:\n",
    "        mlp_data.rename(columns={'Load': 'Hourly_Load_MWh'}, inplace=True)\n",
    "\n",
    "        # Replacing missing or negative loads with NaN:\n",
    "        mlp_data.loc[~(mlp_data['Hourly_Load_MWh'] > 0), 'Hourly_Load_MWh'] = np.nan\n",
    "\n",
    "        # Aggregate the output into a new dataframe:\n",
    "        if file == 0:\n",
    "            tell_df = mlp_data\n",
    "        else:\n",
    "            tell_df = pd.concat([tell_df, mlp_data])\n",
    "    \n",
    "    # Return the output dataframe:\n",
    "    return tell_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c489f3c-66e8-4359-9e02-19057e5d87d3",
   "metadata": {},
   "source": [
    "## Create a Function to Scale the TELL Output Based on the GridView 2035 Values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6412c8be-a4c5-4f58-8026-8ffd637ec7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_tell_loads(data_input_dir: str, tell_data_input_dir: str, year_to_process: str):\n",
    "    \n",
    "    # Aggregate the TELL MLP files:\n",
    "    tell_df = aggregate_mlp_output_files(tell_data_input_dir = tell_data_input_dir,\n",
    "                                         year_to_process = year_to_process)\n",
    "    \n",
    "    # Process the GridView file and rename a column for consistency:\n",
    "    gv_df = process_gridview_data(data_input_dir = data_input_dir)\n",
    "    gv_df.rename(columns={'Total_Load_MWh': 'GV_Total_Load_MWh'}, inplace=True) \n",
    "    \n",
    "    # Merge the tell_df and gv_df dataframes based on common BA names:\n",
    "    merged_df = tell_df.merge(gv_df, on=['BA'])\n",
    "    \n",
    "    # Sum the hourly TELL loads by BA into annual total loads:\n",
    "    merged_df['TELL_Total_Load_MWh'] = merged_df.groupby('BA')['Hourly_Load_MWh'].transform('sum')\n",
    "    \n",
    "    # Compute the scaling factors that force the annual total loads to agree:\n",
    "    merged_df['Scaling_Factor'] = merged_df['GV_Total_Load_MWh'] / merged_df['TELL_Total_Load_MWh']\n",
    "    \n",
    "    # Compute the scaled hourly loads:\n",
    "    merged_df['Hourly_Load_MWh_Scaled'] = merged_df['Hourly_Load_MWh'] * merged_df['Scaling_Factor']\n",
    "    \n",
    "    # Compute the hours since the start of the year:\n",
    "    merged_df['Hour'] = ((pd.to_datetime(merged_df['Time_UTC']) - datetime.datetime(int(year_to_process), 1, 1, 0, 0, 0)) / np.timedelta64(1, 'h') + 1).astype(int)\n",
    "    \n",
    "    # Only keep the columns that are needed:\n",
    "    scaled_tell_df = merged_df[['Hour', 'BA', 'Hourly_Load_MWh_Scaled']].copy()\n",
    "    \n",
    "    # Drop the rows with missing values (i.e., there is not a corresponding GridView load):\n",
    "    scaled_tell_df = scaled_tell_df.dropna(how = 'any')\n",
    "    \n",
    "    # Rename the load variable and round it to 5 decimals:\n",
    "    scaled_tell_df.rename(columns={'Hourly_Load_MWh_Scaled': 'Load_MWh'}, inplace=True)\n",
    "    scaled_tell_df['Load_MWh'] = scaled_tell_df['Load_MWh'].round(5)\n",
    "    \n",
    "    # Return the output dataframe:\n",
    "    return scaled_tell_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79db940a-bc5b-42ca-b1e6-2a97e5d7c009",
   "metadata": {},
   "source": [
    "## Create a Function to Format the Output for Ingest to GridView:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c2d2d2-dadf-45cf-b0e1-40b8856595f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_scaled_tell_loads(data_input_dir: str, tell_data_input_dir: str, year_to_process: str):\n",
    "    \n",
    "    # Aggregate the TELL MLP files:\n",
    "    scaled_tell_df = scale_tell_loads(data_input_dir = data_output_dir,\n",
    "                                      tell_data_input_dir = tell_data_input_dir, \n",
    "                                      year_to_process = year_to_process)\n",
    "    \n",
    "    # Process the GridView file:\n",
    "    gv_df = process_gridview_data(data_input_dir = data_input_dir)\n",
    "    \n",
    "    # Compute the load fractions for the subregions:\n",
    "    CIPB_LF = (gv_df.loc[(gv_df['BA'] == 'CISO_CIPB')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'CISO')]['Total_Load_MWh'].values[0])\n",
    "    CIPV_LF = (gv_df.loc[(gv_df['BA'] == 'CISO_CIPV')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'CISO')]['Total_Load_MWh'].values[0])\n",
    "    CISC_LF = (gv_df.loc[(gv_df['BA'] == 'CISO_CISC')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'CISO')]['Total_Load_MWh'].values[0])\n",
    "    CISD_LF = (gv_df.loc[(gv_df['BA'] == 'CISO_CISD')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'CISO')]['Total_Load_MWh'].values[0])\n",
    "    VEA_LF  = (gv_df.loc[(gv_df['BA'] == 'CISO_VEA' )]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'CISO')]['Total_Load_MWh'].values[0])\n",
    "    IPFE_LF = (gv_df.loc[(gv_df['BA'] == 'IPCO_IPFE')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'IPCO')]['Total_Load_MWh'].values[0])\n",
    "    IPMV_LF = (gv_df.loc[(gv_df['BA'] == 'IPCO_IPMV')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'IPCO')]['Total_Load_MWh'].values[0])\n",
    "    IPTV_LF = (gv_df.loc[(gv_df['BA'] == 'IPCO_IPTV')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'IPCO')]['Total_Load_MWh'].values[0])\n",
    "    NEVP_LF = (gv_df.loc[(gv_df['BA'] == 'NEVP_NEVP')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'NEVP')]['Total_Load_MWh'].values[0])\n",
    "    SPPC_LF = (gv_df.loc[(gv_df['BA'] == 'NEVP_SPPC')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'NEVP')]['Total_Load_MWh'].values[0])\n",
    "    PAID_LF = (gv_df.loc[(gv_df['BA'] == 'PACE_PAID')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'PACE')]['Total_Load_MWh'].values[0])\n",
    "    PAUT_LF = (gv_df.loc[(gv_df['BA'] == 'PACE_PAUT')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'PACE')]['Total_Load_MWh'].values[0])\n",
    "    PAWY_LF = (gv_df.loc[(gv_df['BA'] == 'PACE_PAWY')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'PACE')]['Total_Load_MWh'].values[0])\n",
    "    \n",
    "    # Reshape the dataframe and drop the indexes:\n",
    "    output_df = scaled_tell_df.pivot(index = 'Hour', columns = 'BA', values = 'Load_MWh')\n",
    "    output_df = output_df.reset_index(drop=False)\n",
    "    \n",
    "    # Add back in the text to the column headers:\n",
    "    output_df = output_df.add_suffix(\"_2030.dat\")\n",
    "    output_df = output_df.add_prefix(\"Load_\")\n",
    "    \n",
    "    # Rename the time variable:\n",
    "    output_df.rename(columns={'Load_Hour_2030.dat': 'Index'}, inplace=True)\n",
    "    \n",
    "    # Compute the loads for the subregions:\n",
    "    output_df['Load_CIPB_2030.dat'] = output_df['Load_CISO_2030.dat'] * CIPB_LF\n",
    "    output_df['Load_CIPV_2030.dat'] = output_df['Load_CISO_2030.dat'] * CIPV_LF\n",
    "    output_df['Load_CISC_2030.dat'] = output_df['Load_CISO_2030.dat'] * CISC_LF\n",
    "    output_df['Load_CISD_2030.dat'] = output_df['Load_CISO_2030.dat'] * CISD_LF\n",
    "    output_df['Load_VEA_2030.dat'] = output_df['Load_CISO_2030.dat'] * VEA_LF\n",
    "    output_df['Load_IPFE_2030.dat'] = output_df['Load_IPCO_2030.dat'] * IPFE_LF\n",
    "    output_df['Load_IPMV_2030.dat'] = output_df['Load_IPCO_2030.dat'] * IPMV_LF\n",
    "    output_df['Load_IPTV_2030.dat'] = output_df['Load_IPCO_2030.dat'] * IPTV_LF\n",
    "    output_df['Load_NEVP_Temp_2030.dat'] = output_df['Load_NEVP_2030.dat'] * NEVP_LF\n",
    "    output_df['Load_SPPC_2030.dat'] = output_df['Load_NEVP_2030.dat'] * SPPC_LF\n",
    "    output_df['Load_PAID_2030.dat'] = output_df['Load_PACE_2030.dat'] * PAID_LF\n",
    "    output_df['Load_PAUT_2030.dat'] = output_df['Load_PACE_2030.dat'] * PAUT_LF\n",
    "    output_df['Load_PAWY_2030.dat'] = output_df['Load_PACE_2030.dat'] * PAWY_LF\n",
    "    \n",
    "    # Drop the un-needed columns and clean up the NEVP naming:\n",
    "    del output_df['Load_NEVP_2030.dat'], output_df['Load_CISO_2030.dat'], output_df['Load_IPCO_2030.dat'], output_df['Load_PACE_2030.dat']\n",
    "    output_df.rename(columns={'Load_NEVP_Temp_2030.dat': 'Load_NEVP_2030.dat'}, inplace=True)\n",
    "    \n",
    "    # Add in a blank row and fill it with the year placeholder:\n",
    "    output_df.loc[-0.5] = 0\n",
    "    output_df = output_df.sort_index().reset_index(drop=True)\n",
    "    output_df.iloc[0, :] = '2030'\n",
    "    output_df.at[0, 'Index'] = 'Year'\n",
    "    \n",
    "    # Read in the raw data GridView .csv file and convert the values to floats:\n",
    "    raw_gv_df = pd.read_csv((data_input_dir + '2021_1_Heatwave_Load_stress.csv'))\n",
    "    \n",
    "    # Subset to just the rows we need:\n",
    "    raw_gv_df = raw_gv_df[0:8761]\n",
    "    \n",
    "    # Merge in the GridView columns that aren't modeled by TELL:\n",
    "    output_df = pd.concat([output_df,raw_gv_df['Load_AESO_2030.dat']], axis=1)\n",
    "    output_df = pd.concat([output_df,raw_gv_df['Load_BCHA_2030.dat']], axis=1)\n",
    "    output_df = pd.concat([output_df,raw_gv_df['Load_CFE_2030.dat']], axis=1)\n",
    "    output_df = pd.concat([output_df,raw_gv_df['Load_TH_Malin_2030.dat']], axis=1)\n",
    "    output_df = pd.concat([output_df,raw_gv_df['Load_TH_Mead_2030.dat']], axis=1)\n",
    "    output_df = pd.concat([output_df,raw_gv_df['Load_TH_PV_2030.dat']], axis=1)\n",
    "    \n",
    "    # Sort the data by column name then make the Index column column one:\n",
    "    output_df.rename(columns={'Index': 'AA'}, inplace=True)\n",
    "    output_df = output_df.sort_index(axis = 1)\n",
    "    output_df.rename(columns={'AA': 'Index'}, inplace=True)\n",
    "       \n",
    "    # Set the output filenames:\n",
    "    if year_to_process == '2018':\n",
    "       output_filename = 'TELL_Loads_2021_Based_on_2018_Weather.csv'\n",
    "       \n",
    "    # Write out the dataframe to a .csv file:\n",
    "    output_df.to_csv((os.path.join(data_input_dir, output_filename)), sep=',', index=False)\n",
    "    \n",
    "    # Return the output dataframe:\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789f3188-540a-4057-91d1-0e67f9e9595e",
   "metadata": {},
   "source": [
    "## Call the Necessary Functions to Process the Data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfdc9dd-cac3-4091-a81b-2232bf4b083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = format_scaled_tell_loads(data_input_dir = data_output_dir,\n",
    "                                     tell_data_input_dir = tell_data_input_dir,\n",
    "                                     year_to_process = '2018')\n",
    "\n",
    "output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b56e5f-a4ad-439b-a0d3-2c3f6f32ef40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9.15_std",
   "language": "python",
   "name": "py3.9.15_std"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
